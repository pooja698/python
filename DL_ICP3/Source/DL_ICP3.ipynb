{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  type                                             review label  \\\n",
      "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
      "1           1  test  This is an example of why the majority of acti...   neg   \n",
      "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
      "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
      "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
      "\n",
      "          file  \n",
      "0      0_2.txt  \n",
      "1  10000_4.txt  \n",
      "2  10001_1.txt  \n",
      "3  10002_3.txt  \n",
      "4  10003_3.txt  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imdb_master.csv',encoding='latin-1')\n",
    "print(df.head())\n",
    "sentences = df['review'].values\n",
    "pureSentences = sentences\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving vocabulary of the data\n",
    "sentences = tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
    "input_dim=np.prod(X_train.shape[1:])\n",
    "# Number of features\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pooja\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\pooja\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "75000/75000 [==============================] - 11s 140us/step - loss: 0.8783 - acc: 0.5058 - val_loss: 0.8442 - val_acc: 0.5157\n",
      "Epoch 2/5\n",
      "75000/75000 [==============================] - 8s 113us/step - loss: 0.7933 - acc: 0.5733 - val_loss: 0.8410 - val_acc: 0.5191\n",
      "Epoch 3/5\n",
      "75000/75000 [==============================] - 8s 110us/step - loss: 0.6948 - acc: 0.6711 - val_loss: 0.8669 - val_acc: 0.5119\n",
      "Epoch 4/5\n",
      "75000/75000 [==============================] - 8s 109us/step - loss: 0.5134 - acc: 0.8031 - val_loss: 0.9407 - val_acc: 0.5124\n",
      "Epoch 5/5\n",
      "75000/75000 [==============================] - 8s 109us/step - loss: 0.3022 - acc: 0.9179 - val_loss: 1.0788 - val_acc: 0.4996\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(300, activation='relu',input_dim=input_dim))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history=model.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176408\n",
      "2470\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_review_len= max([len(s.split()) for s in pureSentences])\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "sentencesPre = tokenizer.texts_to_sequences(pureSentences)\n",
    "padded_docs= pad_sequences(sentencesPre,maxlen=max_review_len)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.25, random_state=1000)\n",
    "print(vocab_size)\n",
    "print(max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Flatten\n",
    "m = Sequential()\n",
    "m.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
    "m.add(Flatten())\n",
    "m.add(layers.Dense(300, activation='relu',input_dim=max_review_len))\n",
    "m.add(layers.Dense(3, activation='softmax'))\n",
    "m.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "75000/75000 [==============================] - 305s 4ms/step - loss: 1.0393 - acc: 0.4908 - val_loss: 0.8664 - val_acc: 0.5048\n",
      "Epoch 2/5\n",
      "75000/75000 [==============================] - 305s 4ms/step - loss: 0.8442 - acc: 0.5177 - val_loss: 0.8512 - val_acc: 0.5018\n",
      "Epoch 3/5\n",
      "75000/75000 [==============================] - 304s 4ms/step - loss: 0.7986 - acc: 0.5510 - val_loss: 0.8529 - val_acc: 0.5038\n",
      "Epoch 4/5\n",
      "75000/75000 [==============================] - 614s 8ms/step - loss: 0.7265 - acc: 0.6179 - val_loss: 0.8933 - val_acc: 0.4977\n",
      "Epoch 5/5\n",
      "75000/75000 [==============================] - 320s 4ms/step - loss: 0.6247 - acc: 0.6942 - val_loss: 0.9578 - val_acc: 0.4843\n"
     ]
    }
   ],
   "source": [
    "history1=m.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8485 samples, validate on 2829 samples\n",
      "Epoch 1/5\n",
      "8485/8485 [==============================] - 137s 16ms/step - loss: 10.4392 - acc: 0.0552 - val_loss: 3.0707 - val_acc: 0.0626\n",
      "Epoch 2/5\n",
      "8485/8485 [==============================] - 133s 16ms/step - loss: 2.9882 - acc: 0.0699 - val_loss: 2.9829 - val_acc: 0.0534\n",
      "Epoch 3/5\n",
      "8485/8485 [==============================] - 135s 16ms/step - loss: 2.9569 - acc: 0.0772 - val_loss: 2.9781 - val_acc: 0.0696\n",
      "Epoch 4/5\n",
      "8485/8485 [==============================] - 133s 16ms/step - loss: 2.9289 - acc: 0.0742 - val_loss: 2.9473 - val_acc: 0.0700\n",
      "Epoch 5/5\n",
      "8448/8485 [============================>.] - ETA: 0s - loss: 2.8947 - acc: 0.0814"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "sentences= twenty_train.data\n",
    "y=twenty_train.target\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# sentences = tokenizer.texts_to_matrix(sentences)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_review_len= max([len(s.split()) for s in sentences])\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "sentencesPre = tokenizer.texts_to_sequences(sentences)\n",
    "padded_docs= pad_sequences(sentencesPre,maxlen=max_review_len)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "from keras.layers import Embedding, Flatten\n",
    "n = Sequential()\n",
    "n.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
    "n.add(Flatten())\n",
    "n.add(layers.Dense(300, activation='relu',input_dim=max_review_len))\n",
    "n.add(layers.Dense(20, activation='softmax'))\n",
    "n.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "h2=n.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
